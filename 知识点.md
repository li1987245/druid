### hadoop
#### hdfs
##### NameNode
- 文件结构
edits、fsimage、seen_txid、VERSION
1.edits
编辑日志（edit log）：当客户端执行写操作时，首先NameNode会在编辑日志中写下记录，并在内存中保存一个文件系统元数据，这个描述符会在编辑日志改动之后更新。
edits_start transaction ID-end transaction ID
finalized edit log segments，在HA（高可用）环境中，Standby Namenode只能读取finalized log segments，
edits_inprogress__start transaction ID
当前正在被追加的edit log，HDFS默认会为该文件提前申请1MB空间以提升性能
2.fsimage
文件系统镜像（fsimage）:文件系统元数据的持久检查点，包含以序列化格式（从Hadoop-2.4.0起，FSImage开始采用Google Protobuf编码格式）存储的文件系统目录和文件inodes，每个inodes表征一个文件或目录的元数据信息以及文件的副本数、修改和访问时间等信息。
fsimage_end transaction ID
每次checkpoing（合并所有edits到一个fsimage的过程）产生的最终的fsimage，同时会生成一个.md5的文件用来对文件做完整性校验
3.seen_txid
seen_txid是存放transactionId的文件，format之后是0，它代表的是namenode里面的edits_*文件的尾数，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字
4.VERSION
VERSION文件是java属性文件，保存了HDFS的版本号
• namespaceID是文件系统的唯一标识符，是在文件系统初次格式化时生成的。
• clusterID是系统生成或手动指定的集群ID
• cTime表示NameNode存储时间的创建时间，升级后会更新该值。
• storageType表示此文件夹中保存的是元数据节点的数据结构。
• blockpoolID：针对每一个Namespace所对应blockpool的ID,该ID包括了其对应的NameNode节点的ip地址。
• layoutVersion是一个负整数，保存了HDFS的持续化在硬盘上的数据结构的格式版本号。
5.in_use.lock
防止一台机器同时启动多个Namenode进程导致目录数据不一致
6.namenode同步：
1）Secondary NameNode首先请求原NameNode进行edits的滚动，这样新的编辑操作就能够进入新的文件中。

2）Secondary NameNode通过HTTP方式读取原NameNode中的fsimage及edits。

3）Secondary NameNode读取fsimage到内存中，然后执行edits中的每个操作，并创建一个新的统一的fsimage文件。

4）Secondary NameNode通过HTTP方式将新的fsimage发送到原NameNode。

5）原NameNode用新的fsimage替换旧的fsimage，旧的edits文件用步骤1）中的edits进行替换（将edits.new改名为edits）。同时系统会更新fsimage文件到记录检查点的时间。
这个过程结束后，NameNode就有了最新的fsimage文件和更小的edits文件

注：可执行hadoop dfsadmin –saveNamespace命令运行上图的过程Secondary NameNode（NameNode的冷备份）每隔一小时会插入一个检查点，如果编辑日志达到64MB，则间隔时间更短，每隔5分钟检查一次。
https://blog.csdn.net/baiye_xing/article/details/76268495
##### DataNode
DataNode的文件结构主要由blk_前缀文件、BP-random integer-NameNode-IP address-creation time和VERSION构成
1.BP-random integer-NameNode-IP address-creation time
BP代表BlockPool的，就是Namenode的VERSION中的集群唯一blockpoolID
2.finalized/rbw
这两个目录都是用于实际存储HDFS BLOCK的数据，里面包含许多block_xx文件以及相应的.meta文件，.meta文件包含了checksum信息。
rbw是“replica being written”的意思，该目录用于存储用户当前正在写入的数据。
3.blk_前缀文件
HDFS中的文件块本身，存储的是原始文件内容。
块的元数据信息（使用.meta后缀标识）。一个文件块由存储的原始文件字节组成，元数据文件由一个包含版本和类型信息的头文件和一系列块的区域校验和组成。

- counter
```
2019-02-04 15:58:46,004 INFO mapreduce.Job: Counters: 54
	File System Counters
		FILE: Number of bytes read=653050
		FILE: Number of bytes written=1749879
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=618040
		HDFS: Number of bytes written=624269
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
		HDFS: Number of bytes read erasure-coded=0
	Job Counters
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=5408
		Total time spent by all reduces in occupied slots (ms)=6540
		Total time spent by all map tasks (ms)=2704
		Total time spent by all reduce tasks (ms)=3270
		Total vcore-milliseconds taken by all map tasks=2704
		Total vcore-milliseconds taken by all reduce tasks=3270
		Total megabyte-milliseconds taken by all map tasks=1384448
		Total megabyte-milliseconds taken by all reduce tasks=1674240
	Map-Reduce Framework
		Map input records=4058
		Map output records=5533
		Map output bytes=639356
		Map output materialized bytes=653050
		Input split bytes=116
		Combine input records=5533
		Combine output records=5502
		Reduce input groups=5502
		Reduce shuffle bytes=653050
		Reduce input records=5502
		Reduce output records=5502
		Spilled Records=11004
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=132
		CPU time spent (ms)=1720
		Physical memory (bytes) snapshot=535298048
		Virtual memory (bytes) snapshot=4670066688
		Total committed heap usage (bytes)=406323200
		Peak Map Physical memory (bytes)=296005632
		Peak Map Virtual memory (bytes)=2333917184
		Peak Reduce Physical memory (bytes)=239292416
		Peak Reduce Virtual memory (bytes)=2336149504
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters
		Bytes Read=617924
	File Output Format Counters
		Bytes Written=624269
```
### yarn


### hive
#### DDL
- 创建
```
CREATE EXTERNAL TABLE ods.user (
  user_num STRING COMMENT '用户编号',
  mobile STRING COMMENT '手机号码',
  reg_date STRING COMMENT '注册日期'
COMMENT '用户资料表'
PARTITIONED BY (dt string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'
STORED AS ORC
LOCATION '/ods/user';
)
```
#### 拉链表
- 概念
```
通过数据结束时间来确定数据是否为最新记录，默认结束时间为最大时间，如果数据有修改，设置数据结束时间为修改时间，修改后数据的新增时间为修改时间，结束时间为最大时间
```
- 应用场景
```
在数据仓库的数据模型设计过程中，经常会遇到下面这种表的设计：

有一些表的数据量很大，比如一张用户表，大约10亿条记录，50个字段，这种表，即使使用ORC压缩，单张表的存储也会超过100G，在HDFS使用双备份或者三备份的话就更大一些。
表中的部分字段会被update更新操作，如用户联系方式，产品的描述信息，订单的状态等等。
需要查看某一个时间点或者时间段的历史快照信息，比如，查看某一个订单在历史某一个时间点的状态。
表中的记录变化的比例和频率不是很大，比如，总共有10亿的用户，每天新增和发生变化的有200万左右，变化的比例占的很小。
那么对于这种表我该如何设计呢？下面有几种方案可选：

方案一：每天只留最新的一份，比如我们每天用Sqoop抽取最新的一份全量数据到Hive中。
方案二：每天保留一份全量的切片数据。
方案三：使用拉链表。
```
- 设计和实现
```
1. ODS层原始切片表（按天分区，包含更新时间）
2. 加工更新表（包含新增和被修改的数据）
3. INSERT OVERWRITE TABLE dws.table
   SELECT * FROM
   (
       SELECT A.user_num,
              A.mobile,
              A.reg_date,
              A.t_start_time,
              CASE
                   WHEN A.t_end_time = '9999-12-31' AND B.user_num IS NOT NULL THEN '2017-01-01'
                   ELSE A.t_end_time
              END AS t_end_time
       FROM dws.user_his AS A
       LEFT JOIN ods.user_update AS B
       ON A.user_num = B.user_num
   UNION
       SELECT C.user_num,
              C.mobile,
              C.reg_date,
              '2017-01-02' AS t_start_time,
              '9999-12-31' AS t_end_time
       FROM ods.user_update AS C
   ) AS T
```
- 冷热数据分离
```
因为拉链表会保存历史全量变更，数据量会不断增加，可以抽取最近一段时间的变更数据供后面使用
```

### hbase

- Phoenix

### spark
- scala match
- scala容器

- scala隐式转换
```
1.隐式参数
隐式参数列表，置于方法的最后一个参数列表，方法有多个隐式参数，只需一个implicit修饰即可，def foo(n: Int)(implicit t1: String, t2: Double = 3.14)。
当调用包含隐式参数的方法时，自动在当前上下文中查找合适的隐式值
2.隐式地转换类型
使用隐含转换将变量转换成预期的类型
3.隐式调用函数
隐式调用函数可以转换调用方法的对象，比如但编译器看到X .method，而类型 X 没有定义 method（包括基类)方法，那么编译器就查找作用域内定义的从 X 到其它对象的类型转换
```
- scala偏函数

- scala特质

- spark rdd dataframe dataset区别和转换

- spark shuffle stage
```
reduceByKey、groupByKey、countByKey、join等操作会产生shuffle
# 开启map端输出文件合并
spark.shuffle.consolidateFiles # 默认为false，设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件
new SparkConf().set("spark.shuffle.consolidateFiles", "true")
# shuffle缓存调整
spark.shuffle.file.buffer # 设置shuffle write task的BufferedOutputStream的buffer缓冲大小，默认32k
spark.reducer.maxSizeInFlight # 设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据
spark.shuffle.memoryFraction # 设置Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是0.2
spark.shuffle.manager # 默认值：sort，设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。
                        通过bypass机制（spark.shuffle.sort.bypassMergeThreshold大于shuffle read task的数量。那么此时就会自动启用bypass机制）或优化的HashShuffleManager来避免排序操作
```
- spark常用算子
map flatmap

-spark partition

- spark-submit 调优参数

- spark 内存溢出

- spark cache persist

- spark数据倾斜


### Storm
https://blog.csdn.net/vim_wj/article/details/75831677
### impala

### kafka


### zookeeper


### elasticsearch


### flume


### ranger


### Flink


### Kylin

### MQ
#### rabbitMQ
a)消费者是无法订阅或者获取不存在的MessageQueue中信息。
b)消息被Exchange接受以后，如果没有匹配的Queue，则会被丢弃。
- exchange
```
Exchange是接受生产者消息并将消息路由到消息队列的关键组件。ExchangeType和Binding决定了消息的路由规则。所以生产者想要发送消息，首先必须要声明一个Exchange和该Exchange对应的Binding。
可以通过 ExchangeDeclare和BindingDeclare完成。在Rabbit MQ中，声明一个Exchange需要三个参数：ExchangeName，ExchangeType和Durable。
ExchangeName是该Exchange的名字，该属性在创建Binding和生产者通过publish推送消息时需要指定。ExchangeType，指Exchange的类型，在RabbitMQ中，有三种类型的Exchange：direct ，fanout和topic，不同的Exchange会表现出不同路由行为。
Direct类型，则会将消息中的RoutingKey与该Exchange关联的所有Binding中的BindingKey进行比较，如果相等，则发送到该Binding对应的Queue中。
Fanout  类型，则会将消息发送给所有与该  Exchange  定义过  Binding  的所有  Queues  中去，其实是一种广播行为。
Topic类型，则会按照正则表达式，对RoutingKey(生产者)与BindingKey(bind指定)进行匹配，如果匹配成功，则发送到对应的Queue中。
```
- Binding
```
声明一个Binding需要提供一个QueueName，ExchangeName和BindingKey。
```
- queue


### java
- 集合

- 高并发

- IO
```
connect per reset
对端已经关闭，并返回reset后，还在读取
broken pipe
对端已经关闭，返回reset后，还在写入
```


- 垃圾收集

- ClassLoader
```
两种方法分别是：
1. java -Xbootclasspath/a:/etc/hadoop/conf:/etc/hive/conf -jar example.jar
2. java -cp /etc/hadoop/conf:/etc/hive/conf:./example.jar example.Main.class
注意事项：
（1）-Xbootclasspath/a:要在-jar之前
（2）-Xbootclasspath/a:和后面的参数之间不能有空格
（3）example.Main.class是jar包的主类，要把相应的jar包放到classpath参数中。
（4）文件路径之间使用分隔符（win下为分号，linux下为冒号）

```

- maven
# -pl指定打包模块  -am同时install依赖
mvn clean install -Dmaven.test.skip=true -pl data-insight-manage -am
mvn clean install -Dmaven.test.skip=true -rf data-insight-manage
安装maven helper 插件，在pom.xml文件下点击dependency Analyzer分析查看冲突的包

maven生命周期
```
clean, default, site build lifecycle

clean lifecycle
phase	function
pre-clean execute	execute processes needed prior to the actual project cleaning
clean	remove all files generated by the previous build
post-clean	execute processes needed to finalize the project cleaning
default lifecycle
phase	function
validate	validate the project is correct and all necessary information is available.
initialize	initialize build state, e.g. set properties or create directories.
generate-sources	generate any source code for inclusion in compilation.
process-sources	process the source code, for example to filter any values.
generate-resources	generate resources for inclusion in the package.
process-resources	copy and process the resources into the destination directory, ready for packaging.
compile	compile the source code of the project.
process-classes	post-process the generated files from compilation, for example to do bytecode enhancement on Java classes.
generate-test-sources	generate any test source code for inclusion in compilation.
process-test-sources	process the test source code, for example to filter any values.
generate-test-resources	create resources for testing.
process-test-resources	copy and process the resources into the test destination directory.
test-compile	compile the test source code into the test destination directory
process-test-classes	post-process the generated files from test compilation, for example to do bytecode enhancement on Java classes. For Maven 2.0.5 and above.
test	run tests using a suitable unit testing framework. These tests should not require the code be packaged or deployed.
prepare-package	perform any operations necessary to prepare a package before the actual packaging. This often results in an unpacked, processed version of the package. (Maven 2.1 and above)
package	take the compiled code and package it in its distributable format, such as a JAR.
pre-integration-test perform	actions required before integration tests are executed. This may involve things such as setting up the required environment.
integration-test	process and deploy the package if necessary into an environment where integration tests can be run.
post-integration-test	perform actions required after integration tests have been executed. This may including cleaning up the environment.
verify	run any checks to verify the package is valid and meets quality criteria.
install	install the package into the local repository, for use as a dependency in other projects locally.
deploy	done in an integration or release environment, copies the final package to the remote repository for sharing with other developers and projects.
site lifecycle
phase	function
pre-site	execute processes needed prior to the actual project site generation
site	generate the project's site documentation
post-site	execute processes needed to finalize the site generation, and to prepare for site deployment
site-deploy	deploy the generated site documentation to the specified web server
```
- maven 属性
```
settings.xml文件属性 使用以settings.开头的属性引用,${settings.localRepository}表示本地仓库的地址
使用mvn help:system命令可查看所有的Java系统属性,${user.home}表示用户目录
使用mvn help:system命令可查看所有环境变量,${env.JAVA_HOME}表示JAVA_HOME环境变量的值
```
- 双亲委派
```
web容器需要自定义classloader，实现不同项目之间class隔离
```
- 内存模型

- nio
FileLock实现不同jvm进程之间的文件锁，不能实现同一jvm多线程文件锁，同jvm多线程会抛java.nio.channels.OverlappingFileLockException异常
WatchService监控文件
### netty


### spring cloud
#### zuul
1.cookie丢失
```
1.RibbonRoutingFilter过滤器调用ProxyRequestHelper的buildZuulRequestHeaders方法判断是否过滤header
if (headerNames != null) {
    while (headerNames.hasMoreElements()) {
        String name = headerNames.nextElement();
        //判断是否过滤header
        if (isIncludedHeader(name)) {
            Enumeration<String> values = request.getHeaders(name);
            while (values.hasMoreElements()) {
                String value = values.nextElement();
                headers.add(name, value);
            }
        }
    }
}
2.在PreDecorationFilter过滤器中添加需要过滤的header
if (!route.isCustomSensitiveHeaders()) {
    this.proxyRequestHelper.addIgnoredHeaders(
            this.properties.getSensitiveHeaders().toArray(new String[0]));
}
else {
    this.proxyRequestHelper.addIgnoredHeaders(
            route.getSensitiveHeaders().toArray(new String[0]));
}
3.ZuulProperties中默认设置sensitiveHeaders包括"Cookie", "Set-Cookie", "Authorization"
解决办法：
全局设置：
zuul.sensitive-headers=
指定路由设置：
zuul.routes.<routeName>.sensitive-headers=
zuul.routes.<routeName>.custom-sensitive-headers=true
```
####
1.跨域
```
String origin = request.getHeader("Origin");
response.setHeader("Access-Control-Allow-Origin", origin);
response.setHeader("Access-Control-Allow-Methods", "POST, GET, OPTIONS, PUT, DELETE");
response.setHeader("Access-Control-Max-Age", "3600");
response.setHeader("Access-Control-Allow-Headers",
        "Origin, X-Requested-With, Content-Type, Accept, ticket");
response.setHeader("Access-Control-Allow-Credentials","true");
return super.preHandle(request, response, handler);
```
- dubbo


### k8s+docker
- docker文章
```
http://www.cnblogs.com/SzeCheng/p/6822905.html
```
- Dockerfile配置
1.vim Dockerfile
```
FROM centos:6
# ADD可以自动解压缩
ADD jdk1.8.tar.gz /opt/
ENV JAVA_HOME=
ENV CLASS_PATH=.:$JAVA_HOME/lib:$CLASS_PATH \
PATH=$JAVA_HOME/bin:$PATH
ENV LANG en_US.utf8
```
2.docker build -t jdk:8 -f Dockerfile .

- docker 常用操作
```
# 启动新容器
docker run -itd --name jdk jdk:8 /bin/bash
docker run --name web2 -d -p 81:80 nginx:v2
# 进入运行中的容器
docker exec -it jdk bash
# 停止后台容器
docker container ls -a
docker container stop jdk
# 重启已停止容器
docker container start jdk
# 删除容器
docker container rm jdk
```
- docker安装
```

```

- k8s 组件
```
Delpoyment:
管理rs
ReplicaSet(rs)：
管理pod
Pod：
集群管理最小单位
Service：
逻辑pod组，为一组相同label的pod集合
```

- k8s 常用操作
```
kubectl cluster-info #查询k8s集群信息
kubectl -s http://localhost:8080 get componentstatuses  #查看各组件状态
kubectl get nodes #查看节点
kubectl get rc,namespace #查看rc和namespace
kubectl get pods,svc --all-namespaces #查看pod和svc
kubectl get ingress
kubectl get po mysql -o json #以json格式输出pod的详细信息
kubectl get po mysql -o wide  #查看指定pod跑在哪个node上
kubectl describe pod data-insight-auth-d5cc58f4f-54k7b #查询pod状态信息
kubectl create -f filename #创建文件内定义的resource
kubectl replace -f rc-nginx.yaml #对已有资源进行更新、替换
kubectl edit po mysql #编辑现有的resource
kubectl delete -f rc-nginx.yaml/kubectl delete po rc-nginx-btv4j #删除现有资源
kubectl logs rc-nginx-2-kpiqt #打印容器内程序输出到标准输出的内容
kubectl rolling-update rc-nginx-2 -f rc-nginx.yaml #滚动升级
kubectl scale rc rc-nginx-3 —replicas=4 #调整实例数量
kubectl autoscale rc rc-nginx-3 —min=1 —max=4 #动态调整实例数量
kubectl attach kube-dns-v9-rcfuk -c skydns —namespace=kube-system #直接查看容器中以daemon形式运行的进程的输出，有多个容器，需要使用-c选项指定容器
kubectl exec -it jdk bash #类似于docker的exec命令，有多个容器，需要使用-c选项指定容器

```

### mybatis

- 拦截器


#### 算法
- 图

- bitmap

- hash

- 树

- 排序

- 背包

### redis
- redis value size
```
通信缓冲区的最终限制。
当GET命令应用于大对象时，首先序列化该对象
在通信缓冲区中，然后写入客户端套接字。
最理想的情况是在一个以太网数据（1500byte）包内传输
```
### mysql
- mysql 占用内存过大排查
```
1.通过show full processlist和show open tables查看当前应用的表及正在使用的sql
show global status like 'Open%tables'可以看到历史打开表数量合计（opened_tables）
2.table_open_cache控制table cache总数量
当缓冲已满，而连接想要打开一个不在缓冲中的表时。
当缓冲数目已经超过了table_open_cache设置的值，mysql开始使用LRU算法释放表对象。
当你用flush tables;语句时。
3.open_files_limit表示mysqld可用的最大文件描述符数目，在Unix系统下这个值的数目不能大于ulimit -n
4.innodb_open_files只对InnoDB存储引擎有效，它指定了mysql可以同时打开的最大.ibd文件的数目。
max_connections*你的表数目 = table_open_cache <=open_files_limit< ulimit -n
innodb_open_files<ulimit -n
5.key_buffer_size
show variables like  'key_buffer_size';
show status like 'key_read%'; //Key_read_requests和Key_reads，比例key_reads / key_read_requests应该尽可能的低
6.innodb_buffer_pool_size
缓存innodb表的索引，数据，插入数据时的缓冲
7.query_cache_size
查询缓存
8.tmp_table_size
临时表大小
9.sort_buffer_size
排序缓存
参考：
https://www.cnblogs.com/leohahah/p/8921107.html
https://www.cnblogs.com/kevingrace/p/6133818.html
```
- mysql order by
```
mysql order by 在5.6版本(5.7也存在)当排序的字段值相同时，会出现随机排序的问题

在MySQL 5.6的版本上，优化器在遇到order by limit语句的时候，做了一个优化，即使用了priority queue。
使用 priority queue 的目的，就是在不能使用索引有序性的时候，如果要排序，并且使用了limit n，那么只需要在排序的过程中，保留n条记录即可，这样虽然不能解决所有记录都需要排序的开销，但是只需要 sort buffer 少量的内存就可以完成排序。
之所以MySQL 5.6出现了第二页数据重复的问题，是因为 priority queue 使用了堆排序的排序方法，而堆排序是一个不稳定的排序方法，也就是相同的值可能排序出来的结果和读出来的数据顺序不一致。
MySQL 5.5 没有这个优化，所以也就不会出现这个问题（order by当值相同时也会有问题）。
也就是说，MySQL 5.5是不存在本文提到的问题的，5.6版本之后才出现了这种情况。
再看下MySQL解释sql语言时的执行顺序：
(1)     SELECT
(2)     DISTINCT <select_list>
(3)     FROM <left_table>
(4)     <join_type> JOIN <right_table>
(5)     ON <join_condition>
(6)     WHERE <where_condition>
(7)     GROUP BY <group_by_list>
(8)     HAVING <having_condition>
(9)     ORDER BY <order_by_condition>
(10)    LIMIT <limit_number>
复制代码执行顺序依次为 form… where… select… order by… limit…，由于上述priority queue的原因，在完成select之后，所有记录是以堆排序的方法排列的，在进行order by时，仅把view_count值大的往前移动。但由于limit的因素，排序过程中只需要保留到5条记录即可，view_count并不具备索引有序性，所以当第二页数据要展示时，mysql见到哪一条就拿哪一条，因此，当排序值相同的时候，第一次排序是随意排的，第二次再执行该sql的时候，其结果应该和第一次结果一样。
2 解决方法


索引排序字段

如果在字段添加上索引，就直接按照索引的有序性进行读取并分页，从而可以规避遇到的这个问题。



正确理解分页

分页是建立在排序的基础上，进行了数量范围分割。排序是数据库提供的功能，而分页却是衍生的出来的应用需求。
在MySQL和Oracle的官方文档中提供了limit n和rownum < n的方法，但却没有明确的定义分页这个概念。还有重要的一点，虽然上面的解决方法可以缓解用户的这个问题，但按照用户的理解，依然还有问题：比如，这个表插入比较频繁，用户查询的时候，在read-committed的隔离级别下，第一页和第二页仍然会有重合。
所以，分页一直都有这个问题，不同场景对数据分页都没有非常高的准确性要求。



一些常见的数据库排序问题
不加order by的时候的排序问题

用户在使用Oracle或MySQL的时候，发现MySQL总是有序的，Oracle却很混乱，这个主要是因为Oracle是堆表，MySQL是索引聚簇表的原因。所以没有order by的时候，数据库并不保证记录返回的顺序性，并且不保证每次返回都一致的。

分页问题 分页重复的问题

如前面所描述的，分页是在数据库提供的排序功能的基础上，衍生出来的应用需求，数据库并不保证分页的重复问题。

NULL值和空串问题

不同的数据库对于NULL值和空串的理解和处理是不一样的，比如Oracle NULL和NULL值是无法比较的，既不是相等也不是不相等，是未知的。而对于空串，在插入的时候，MySQL是一个字符串长度为0的空串，而Oracle则直接进行NULL值处理。

作者：猿码道
链接：https://juejin.im/post/5af9537bf265da0b9e652dea
来源：掘金
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
```
- mysql join
```
Nested-Loop Join Algorithms
1.Simple Nested-Loop Join
r为驱动表，s为匹配表，对s表进行了rn次访问
2.Index Nested-Loop Join
要求非驱动表（匹配表s）上有索引，可以通过索引来减少比较，加速查询
驱动表（r）会根据关联字段的索引进行查找，当在索引上找到符合的值，再回表进行查询，也就是只有当匹配到索引以后才会进行回表查询
3.Block Nested-Loop Join Algorithm
如果有索引，会选取第二种方式进行join，但如果join列没有索引，就会采用Block Nested-Loop Join。可以看到中间有个join buffer缓冲区，是将驱动表的所有join相关的列都先缓存到join buffer中，然后批量与匹配表进行匹配，将第一种多次比较合并为一次，降低了非驱动表（s）的访问频率。默认情况下join_buffer_size=256K，在查找的时候MySQL会将所有的需要的列缓存到join buffer当中，包括select的列，而不是仅仅只缓存关联列。在一个有N个JOIN关联的SQL当中会在执行时候分配N-1个join buffer

NLJ扫描驱动表，每读到一条记录，就根据 join 字段上的索引去另一张表（内表）里查找。内表（一般是带索引的表）被驱动表(不要求有索引）驱动，外表(驱动表)返回的每一行都要在内表中检索与其匹配的行。
eg:
Table Join Type:t1 range、t2 ref、t3 ALL
NLJ算法：
扫描总行数为t1 rows*t2 rows *t3 rows（explain中显示的rows），内循环的表会被扫描多次
for each row in t1 matching range {
    for each row in t2 matching reference key {
        for each row in t3 {
            if row satisfies join conditions,
                send to client
        }
    }
}
BLJ算法：
设S是每次存储t1、t2组合的大小，C是组合的数量，则t3被扫描的次数为：
(S * C)/join_buffer_size + 1
由此可见，随着join_buffer_size的增大，t3被扫描的次数会较少，如果join_buffer_size足够大，大到可以容纳所有t1和t2联接产生的数据，t3只会被扫描1次。
for each row in t1 matching range {
    for each row in t2 matching reference key {
        store used columns from t1, t2 in join buffer
        if buffer is full {
            for each row in t3 {
                for each t1, t2 combination in join buffer {
                    if row satisfies join conditions,
                        send to client
                }
            }
            empty buffer
        }
    }
}

if buffer is not empty {
    for each row in t3 {
        for each t1, t2 combination in join buffer {
            if row satisfies join conditions,
                send to client
        }
    }
}

STRAIGHT_JOIN只适用于inner join，作用是强制使用左边的表来驱动右边的表


数据库关联查询算法有Nested Loopsb Join、Merge Join、Hash Join，mysql仅支持nlj
Hash join的主要资源消耗在于CPU(在内存中创建临时的hash表，并进行hash计算)，而merge join的资源消耗主要在于磁盘I/O(扫描表或索引)。在并行系统中，hash join对CPU的消耗更加明显。所以在CPU紧张时，最好限制使用hash join。
```
- mysql in exists
```
in 和exists
in是把外表和内表作hash 连接，而exists 是对外表作loop 循环，每次loop 循环再对内表进行查询。
5.5以后的MySQL版本在exists匹配查询结果时使用的是Block Nested-Loop（Block嵌套循环，引入join buffer，类似于缓存功能）
IN查询在内部表和外部表上都可以使用到索引；
Exists查询仅在内部表上可以使用到索引；
当子查询结果集很大，而外部表较小的时候，Exists的Block Nested Loop(Block 嵌套循环)的作用开始显现，并弥补外部表无法用到索引的缺陷，查询效率会优于IN。
当子查询结果集较小，而外部表很大的时候，Exists的Block嵌套循环优化效果不明显，IN 的外表索引优势占主要作用，此时IN的查询效率会优于Exists。
```
- mysql explain
```
EXPLAIN 结果中，第一行出现的表就是驱动表
对驱动表可以直接排序，对非驱动表（的字段排序）需要对循环查询的合并结果（临时表）进行排序（Important!），即using temporary;
[驱动表] 的定义为：1）满足查询条件的记录行数少的表为[驱动表]；2）未指定查询条件时，行数少的表为[驱动表]（Important!）。
优化的目标是尽可能减少JOIN中Nested Loop的循环次数，以此保证：永远用小结果集驱动大结果集（Important!）！：A JOIN B，A为驱动，A中每一行和B进行循环JOIN，看是否满足条件，所以当A为小结果集时，越快。
NestedLoopJoin实际上就是通过驱动表的结果集作为循环基础数据，然后一条一条的通过该结果集中的数据作为过滤条件到下一个表中查询数据，然后合并结果。
如果还有第三个参与Join，则再通过前两个表的Join结果集作为循环基础数据，再一次通过循环查询条件到第三个表中查询数据，如此往复。
```
#### Canal日志同步

### 缓存


### nginx
1. location ^~ /analysis/不生效

2.rewrite
```
Syntax: 	rewrite regex replacement [flag];
Default: 	—
Context: 	server, location, if

last
    stops processing the current set of ngx_http_rewrite_module directives and starts a search for a new location matching the changed URI;
    本条规则匹配完成后，继续向下匹配新的location URI规则
break
    stops processing the current set of ngx_http_rewrite_module directives as with the break directive;
    本条规则匹配完成即终止，不再匹配后面的任何规则
redirect
    returns a temporary redirect with the 302 code; used if a replacement string does not start with “http://”, “https://”, or “$scheme”;
    返回302临时重定向，浏览器地址会显示跳转后的URL地址
permanent
    returns a permanent redirect with the 301 code.
    返回301永久重定向，浏览器地址栏会显示跳转后的URL地址

rewrite ^(/api/analysis/.*)$ /api/data-insight-analysis$1 last;
```

### git
```
1.git配置ssh
```
git config --global user.name "li_1987245"
git config --global user.email "li_1987245@163.com"
ssh-keygen -t rsa -C "li_1987245@163.com"
```
1.重设账号密码的：
（1）先重新设置本机git配置：git config --global credential.helper store
（2）输入github账号和密码
（3）最后push代码：git push -u origin master
2.重置git配置的http或https代理的：
git config --global --unset http.proxy
git config --global --unset https.proxy
3.Unknown SSL protocol error in connection to github.com:443
git config http.sslVerify "false"
```

### 时间序列分析
- How do we mark every period with all of those that come before it?
```
A proper data modeling will help you answer this kinds of questions with:
YTD – From beginning of the Year until day of Today
QTD – From beginning of the Quarter until day of Today
MTD – From beginning of the Month until day of Today
```